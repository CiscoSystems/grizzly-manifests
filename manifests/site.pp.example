# This document serves as an example of how to deploy
# basic multi-node openstack environments.
# In this scenario Quantum is using OVS with GRE Tunnels
# Swift is not included.

########### Proxy Configuration ##########
# If you use an HTTP/HTTPS proxy, uncomment this setting and specify the correct proxy URL.
# If you do not use an HTTP/HTTPS proxy, leave this setting commented out.
#$proxy			= "http://proxy-server:port-number"

########### package repo configuration ##########
#
# The package repos used to install openstack
$package_repo = 'cisco_repo'
# Alternatively, the upstream Ubuntu package from cloud archive can be used
# $package_repo = 'cloud_archive'

# If you are behind a proxy you may choose not to use our ftp distribution, and
# instead try our http distribution location. Note the http location is not
# a permanent location and may change at any time.
#$location 		= "http://128.107.252.163/openstack/cisco"
# Alternate, uncomment this one, and comment out the one above
$location               = "http://apt.ctocllab.cisco.com/repos/n1k-test"
########### Build Node (Cobbler, Puppet Master, NTP) ######
# Change the following to the host name you have given your build node.
# This name should be in all lower case letters due to a Puppet limitation
# (refer to http://projects.puppetlabs.com/issues/1168).
$build_node_name        = "savbu-osbuild-srv"

########### NTP Configuration ############
# Change this to the location of a time server in your organization accessible to the build server
# The build server will synchronize with this time server, and will in turn function as the time
# server for your OpenStack nodes
$ntp_servers	= ["ntp.esl.cisco.com"]

########### Build Node Cobbler Variables ############
# Change these 5 parameters to define the IP address and other network settings of your build node
# The cobbler node *must* have this IP configured and it *must* be on the same network as
# the hosts to install
$cobbler_node_ip        = '51.1.1.9'
$node_subnet            = '62.1.151.0'
$node_netmask           = '255.255.255.0'
# This gateway is optional - if there's a gateway providing a default route, put it here
# If not, comment it out
$node_gateway           = '62.1.151.254'

$dhcp_subnet  = "62.1.151.0"
$dhcp_netmask = "255.255.255.0"
$dhcp_gateway = "62.1.151.254"

# This domain name will be the name your build and compute nodes use for the local DNS
# It doesn't have to be the name of your corporate DNS - a local DNS server on the build
# node will serve addresses in this domain - but if it is, you can also add entries for
# the nodes in your corporate DNS environment they will be usable *if* the above addresses
# are routeable from elsewhere in your network.
$domain_name 		= 'cisco.com'
# This setting likely does not need to be changed
# To speed installation of your OpenStack nodes, it configures your build node to function
# as a caching proxy storing the Ubuntu install files used to deploy the OpenStack nodes
$cobbler_proxy 		= "http://${cobbler_node_ip}:3142/"

####### Preseed File Configuration #######
# This will build a preseed file called 'cisco-preseed' in /etc/cobbler/preseeds/
# The preseed file automates the installation of Ubuntu onto the OpenStack nodes
#
# The following variables may be changed by the system admin:
# 1) admin_user
# 2) password_crypted
# 3) autostart_puppet -- whether the puppet agent will auto start
# Default user is: localadmin 
# Default SHA-512 hashed password is "ubuntu": $6$UfgWxrIv$k4KfzAEMqMg.fppmSOTd0usI4j6gfjs0962.JXsoJRWa5wMz8yQk4SfInn4.WZ3L/MCt5u.62tHDGB36EhiKF1
# To generate a new SHA-512 hashed password, run the following replacing
# the word "password" with your new password. Then use the result as the
# $password_crypted variable
# python -c "import crypt, getpass, pwd; print crypt.crypt('password', '\$6\$UfgWxrIv\$')"
$admin_user 		= 'localadmin'
$password_crypted 	= '$6$UfgWxrIv$k4KfzAEMqMg.fppmSOTd0usI4j6gfjs0962.JXsoJRWa5wMz8yQk4SfInn4.WZ3L/MCt5u.62tHDGB36EhiKF1'
$autostart_puppet       = true

# If the setup uses the UCS Bseries blades, enter the port on which the
# ucsm accepts requests. By default the UCSM is enabled to accept requests
# on port 443 (https). If https is disabled and only http is used, set
# $ucsm_port = '80'
$ucsm_port = '443'

########### OpenStack Variables ############
# These values define parameters which will be used to deploy and configure OpenStack
# once Ubuntu is installed on your nodes
#
# Change these next 3 parameters to the network settings of the node which 
# will be your OpenStack control node.  Note that the $controller_hostname
# should be in all lowercase letters due to a limitation of Puppet
# (refer to http://projects.puppetlabs.com/issues/1168).
$controller_node_address       = '62.1.151.2'
$controller_node_network       = '62.1.151.0'
$controller_hostname           = 'ostack-controller'
# Specify the network which should have access to the MySQL database on the OpenStack control
# node. Typically, this will be the same network as defined in the controller_node_network
# parameter above. Use MySQL network wild card syntax to specify the desired network.
$db_allowed_network            = '%'
# These next two values typically do not need to be changed. They define the network connectivity
# of the OpenStack controller
# This is the interface used to connect to Horizon dashboard
$controller_node_public        = $controller_node_address
# This is the interface used for external backend communication
$controller_node_internal      = $controller_node_address

# These next three parameters specify the networking hardware used in each node
# Current assumption is that all nodes have the same network interfaces and are
# cabled identically
#
# public_interface is the interface that each service's API listens on
#   it's also commonly referred to as the management interface
$public_interface        	= 'eth2'
# private_interface is the interface used for network connectivity between VMs.
#   specifying and interface allows you to create a network specifically for GRE 
#   tunneled traffic between compute and network nodes.
$private_interface		= 'eth0'
# external_interface is used for external connectivity such as floating IPs (only in network/controller node)
$external_interface	 	= 'eth3'

# Select the drive on which Ubuntu and OpenStack will be installed in each node. Current assumption is
# that all nodes will be installed on the same device name
$install_drive           = '/dev/sda'

########### OpenStack Service Credentials ############
# This block of parameters is used to change the user names and passwords used by the services which
# make up OpenStack. The following defaults should be changed for any production deployment
$admin_email             = 'root@localhost'
$admin_password          = 'n1k12345'
$keystone_db_password    = 'n1k12345'
$keystone_admin_token    = 'n1k12345'
$nova_user               = 'nova'
$nova_db_password        = 'n1k12345'
$nova_user_password      = 'n1k12345'
$glance_db_password      = 'n1k12345'
$glance_user_password    = 'n1k12345'
$glance_sql_connection   = "mysql://glance:${glance_db_password}@${controller_node_address}/glance"
$glance_on_swift         = false
$rabbit_password         = 'n1k12345'
$rabbit_user             = 'n1kv'
# Nova DB connection
$sql_connection          = "mysql://${nova_user}:${nova_db_password}@${controller_node_address}/nova"

$libvirt_type            = 'kvm'
$cinder_user_password    = 'n1k12345'
$cinder_db_password      = 'n1k12345'
$quantum_user_password   = 'n1k12345'
$quantum_db_password     = 'n1k12345'
# glance backend configuration, supports file or swift
$glance_backend = 'file'

########### Test variables ############
# variables used to populate test script:
# /tmp/test_nova.sh
#
# image to use for tests. Accepts kvm or cirros
$test_file_image_type = 'kvm'

# VSM related shared variables
$n1kinstall    = true
$vemimage      = "/home/n1kv/nexus_1000v_vem-12.04-5.2.1.SK1.1.0.348.S0-29.deb"
$vsmimage      = "/home/n1kv/n1000v-dk9.5.2.1.SK1.1.0.348.iso"
$vsmip         = "62.1.151.102"
$domainid      = "111"
$ctrlmac       = "00:02:3d:72:40:00"
$hostmgmtint   = "eth2"
$uplinkint     = "eth3"

#### end shared variables #################

# Storage Configuration
# Set to true to enable Cinder services
$cinder_controller_enabled     = true

# Set to true to enable Cinder deployment to all compute nodes
$cinder_compute_enabled        = true

# The cinder storage driver to use. Default is iscsi
$cinder_storage_driver         = 'iscsi'

# Other drivers exist for cinder. Here are examples on how to enable them.
#
# NetApp iSCSI Driver
# $cinder_storage_driver = 'netapp'
# $netapp_wsdl_url       = ''
# $netapp_login          = ''
# $netapp_password       = ''
#
# NFS
# share information is stored in flat text file specified in $nfs_shares_config
# the format for this file is hostname:/mountpoint eg 192.168.2.55:/myshare, with only one entry per line
#
# $cinder_storage_driver = 'nfs'
# $nfs_shares_config     = '/etc/cinder/shares.conf'



####### OpenStack Node Definitions #####
# This section is used to define the hardware parameters of the nodes which will be used
# for OpenStack. Cobbler will automate the installation of Ubuntu onto these nodes using
# these settings

node /build-node/ inherits master-node {

# This block defines the control server. Replace "control_server" with the 
# host name of your OpenStack controller, and change the "mac" to the MAC 
# address of the boot interface of your OpenStack controller. Change the 
#"ip" to the IP address of your OpenStack controller.

  cobbler_node { "ostack-controller":
    node_type => "control",
    mac => "d4:8c:b5:5e:35:5a",
    ip => "62.1.151.2",
    power_address  => "10.193.180.44",
  }

# This block defines the first compute server. Replace "compute_server01" 
# with the host name of your first OpenStack compute node (note: the hostname
# should be in all lowercase letters due to a limitation of Puppet; refer to
# http://projects.puppetlabs.com/issues/1168), and change the "mac" to the 
# MAC address of the boot interface of your first OpenStack compute node. 
# Change the "ip" to the IP address of your first OpenStack compute node.

# Begin compute node
  cobbler_node { "compute-server01":
    node_type => "compute",
    mac => "d4:8c:b5:4e:80:c0",
    ip => "62.1.151.3",
    power_address  => "10.193.180.48",
  }

  cobbler_node { "compute-server02":
    node_type => "compute",
    mac => "d4:8c:b5:4e:25:4a",
    ip => "62.1.151.4",
    power_address  => "10.193.180.47",
  }

  cobbler_node { "kvm-host-1":
    node_type => "compute",
    mac => "30:f7:0d:47:a3:c6",
    ip => "62.1.151.100",
    power_address  => "10.193.180.49",
  }

  cobbler_node { "kvm-host-2":
    node_type => "compute",
    mac => "d4:8c:b5:4e:2e:16",
    ip => "62.1.151.101",
    power_address  => "10.193.180.50",
  }

# Example with UCS blade power_address with a sub-group (in UCSM), and a ServiceProfile for power_id
# you will need to change power type to 'USC' in the define macro above
#  cobbler_node { "compute-server01":
#    node_type => "compute",
#    mac => "11:22:33:44:66:77",
#    ip => "192.168.242.21",
#    power_address  => "192.168.242.121:org-cisco",
#    power_id => "OpenStack-1"
#  }
# End compute node

### Repeat as needed ###
# Make a copy of your compute node block above for each additional OpenStack node in your cluster
# and paste the copy in this section. Be sure to change the host name, mac, ip, and power settings
# for each node


### End repeated nodes ###
}

### Node types ###
# These lines specify the host names in your OpenStack cluster and what the function of each host is
# internal_ip should be the same as what is specified as "ip" in the OpenStack node definitions above.
#   this sets the IP for the private(internal) interface of controller nodes (which is predefined already in $controller_node_internal, and the internal interface for 
#   compute nodes.
#   In this example, eth0 is both the public and private interface for the controller.
# tunnel_ip allows you to create a network specifically for GRE tunneled traffic between compute and 
#   network nodes. Generally, you will want to use "ip" from the OpenStack node definitions above.
#   This sets the IP for the private interface of compute and network nodes.

# Change build_server to the host name of your build node
node savbu-osbuild-srv inherits build-node { }

# Change control_server to the host name of your control node.  Note that the
# hostname should be in all lowercase letters due to a limitation of Puppet
# (refer to http://projects.puppetlabs.com/issues/1168).
node ostack-controller inherits os_base {
  class { 'control':
    tunnel_ip   => '62.1.151.2'
  }
}

# Change compute_server01 to the host name of your first compute node
node compute-server01 inherits os_base {
  class { 'compute':
    internal_ip => '62.1.151.3',
    tunnel_ip   => '62.1.151.3',
  }

  class {"n1k-vem":
         vemimage => $vemimage,
         vsmip => $vsmip,
         domainid => $domainid,
         ctrlmac => $ctrlmac,
         hostmgmtint => $hostmgmtint,
         uplinkint => $uplinkint,
         profile => 'sys-uplink',
         vtepconfig => 'vmknic-int1 profint mode dhcp mac 00:11:22:33:44:66, vmknic-int2 profint mode static address 192.168.1.91 netmask 255.255.255.0',
         n1kconfname => "cs1",
 }
}

# Change compute_server02 to the host name of your first compute node
node compute-server02 inherits os_base {
  class { 'compute':
    internal_ip => '62.1.151.4',
    tunnel_ip   => '62.1.151.4',
  }

  class {"n1k-vem":
         vemimage => $vemimage,
         vsmip => $vsmip,
         domainid => $domainid,
         ctrlmac => $ctrlmac,
         hostmgmtint => $hostmgmtint,
         uplinkint => $uplinkint,
         profile => 'sys-uplink',
         vtepconfig => 'vmknic-int1 profint mode dhcp mac 00:11:22:33:44:66, vmknic-int2 profint mode static address 192.168.1.92 netmask 255.255.255.0',
         n1kconfname => "cs2",
 }
}

node kvm-host-1 {
     class { n1k-vsm:
          # if you have a host with kvm/ovs configured already where you want to bring up
          # the vsm, then comment the next 6 lines
          configureovs => true,
          ovsbridge => "br0",
          physicalinterfaceforovs => "eth2",
          nodeip => "62.1.151.100",
          nodenetmask => "255.255.255.0",
          nodegateway => "62.1.151.254",
          ### The commented out parameters have a default value as specified, if you want it to be
          ### different uncomment the parameter and change the value as required
          ### memory is specified in KiB and disk size is in GB.

          #cpu => '1',
          #memory => '2048000',
          #disksize => '4'
          #consolepts => '2',

          vsmname => 'vsm-p',
          role => 'primary',
          isoimage => $::vsmimage,
          domainid => $::domainid,
          adminpasswd => 'N1k12345',
          mgmtip => $::vsmip,
          mgmtnetmask => '255.255.255.0',
          mgmtgateway => '62.1.151.254',
          ctrlinterface => ['tap0', $::ctrlmac, 'br0' ],
          mgmtinterface => ['tap1', '00:02:3d:72:40:02', 'br0'],
          pktinterface => ['tap2', '00:02:3d:72:40:03', 'br0']
     }
}

node kvm-host-2 {
     class { n1k-vsm:
          configureovs => true,
          ovsbridge => "br0",
          physicalinterfaceforovs => "eth2",
          nodeip => "62.1.151.101",
          nodenetmask => "255.255.255.0",
          nodegateway => "62.1.151.254",
          ####
          vsmname => 'vsm-s',
          role => 'secondary',
          isoimage => $::vsmimage,
          domainid => $::domainid,
          adminpasswd => 'N1k12345',
          mgmtip => '0.0.0.0',
          mgmtnetmask => '0.0.0.0',
          mgmtgateway => '0.0.0.0',
          ctrlinterface => ['tap0', '00:02:4d:72:40:01', 'br0' ],
          mgmtinterface => ['tap1', '00:02:4d:72:40:02', 'br0'],
          pktinterface => ['tap2', '00:02:4d:72:40:03', 'br0']
     }
}

### Repeat as needed ###
# Copy the compute_server01 line above and paste a copy here for each additional OpenStack node in
# your cluster. Be sure to replace the compute_server01 parameter with the correct host name for
# each additional node

### End repeated nodes ###

########################################################################
### All parameters below this point likely do not need to be changed ###
########################################################################

### Advanced Users Configuration ###
# These four settings typically do not need to be changed
# In the default deployment, the build node functions as the DNS and static DHCP server for
# the OpenStack nodes. These settings can be used if alternate configurations are needed
$node_dns       = "${cobbler_node_ip}"
$ip 		= "${cobbler_node_ip}"
$dns_service 	= "dnsmasq"
$dhcp_service 	= "dnsmasq"
$time_zone      = "UTC"

# Enable network interface bonding. This will only enable the bonding module in the OS, 
# it won't acutally bond any interfaces. Edit the networking interfaces template to set 
# up interface bonds as required after setting this to true should bonding be required.
#$interface_bonding = 'true' 

# Enable ipv6 router edvertisement
#$ipv6_ra = '1'

# Configure the maximum number of times mysql-server will allow
# a host to fail connecting before banning it
$max_connect_errors = '10'

### Puppet Parameters ###
# These settings load other puppet components. They should not be changed
import 'cobbler-node'
import 'core'

## Define the default node, to capture any un-defined nodes that register
## Simplifies debug when necessary.

node default {
  notify{"Default Node: Perhaps add a node definition to site.pp": }
}
